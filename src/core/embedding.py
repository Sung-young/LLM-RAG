import os
import io
import logging
import torch
from datetime import datetime
from tqdm import tqdm
from src.handler.document_loader import CustomDocumentLoader
from src.handler.hybrid_llm_document_loader import HybridLLMPdfLoader
from src.handler.new_document_loader import PdfLoader
from langchain_openai import OpenAIEmbeddings
from sentence_transformers import SentenceTransformer
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
torch.mps.empty_cache()

# Embedding ëª¨ë¸ ì„¤ì • 
# embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
model_name = "dragonkue/bge-m3-ko"
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={"device": "cpu"}   
)


def append_to_vectorstore(input_path: str, index_path: str = "faiss_index", batch_size: int = 500):
    """í´ë” ë‚´ íŒŒì¼ì„ ë²¡í„°DBì— ì¶”ê°€í•˜ë˜, ì´ë¯¸ ì„ë² ë”©ëœ íŒŒì¼ì€ ê±´ë„ˆëœ€."""
    all_documents = []

    # í´ë”/íŒŒì¼ íƒìƒ‰
    file_list = []
    skipped_count = 0
    if os.path.isdir(input_path):
        for root, _, files in os.walk(input_path):
            for file in files:
                # macOS ë©”íƒ€ë°ì´í„° íŒŒì¼ í•„í„°ë§ (._ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼)
                if file.startswith("._") or file.startswith(".DS_Store"):
                    skipped_count += 1
                    continue
                # ìˆ¨ê¹€ íŒŒì¼ í•„í„°ë§ (ì ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼)
                if file.startswith("."):
                    skipped_count += 1
                    continue
                if file.lower().endswith((".pdf", ".xlsx", ".xls", ".txt", ".csv", ".docx")):
                    file_list.append(os.path.join(root, file))
    elif os.path.isfile(input_path):
        # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°ì—ë„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì²´í¬
        basename = os.path.basename(input_path)
        if not (basename.startswith("._") or basename.startswith(".DS_Store") or basename.startswith(".")):
            file_list.append(input_path)
        else:
            logging.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì€ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
            return
    else:
        logging.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤: {input_path}")
        return
    
    if skipped_count > 0:
        logging.info(f"ë©”íƒ€ë°ì´í„°/ìˆ¨ê¹€ íŒŒì¼ {skipped_count}ê°œë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")

    if not file_list:
        logging.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì´ {len(file_list)}ê°œì˜ íŒŒì¼ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")

    # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ 
    existing_sources = set()
    os.makedirs(index_path, exist_ok=True)
    faiss_file = os.path.join(index_path, "index.faiss")
    pkl_file = os.path.join(index_path, "index.pkl")

    vectorstore = None
    if os.path.exists(faiss_file) and os.path.exists(pkl_file):
        logging.info("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
        # ê¸°ì¡´ ë¬¸ì„œë“¤ì˜ 'source' ê²½ë¡œ ì¶”ì¶œ
        existing_sources = {doc.metadata.get("source") for doc in vectorstore.docstore._dict.values()}
        logging.info(f"ì´ë¯¸ ì„ë² ë”©ëœ íŒŒì¼ {len(existing_sources)}ê°œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

    # ìƒˆë¡œ ì¶”ê°€í•  íŒŒì¼ë§Œ í•„í„°ë§
    new_files = [f for f in file_list if f not in existing_sources]
    if not new_files:
        logging.info("ìƒˆë¡œ ì¶”ê°€í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì„ë² ë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ìƒˆë¡œ ì„ë² ë”©í•  íŒŒì¼ {len(new_files)}ê°œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # ìƒˆ íŒŒì¼ ì„ë² ë”©
    success_count = 0
    error_count = 0
    error_files = []
    
    for path in tqdm(new_files, desc="ìƒˆ ë¬¸ì„œ ë¡œë”© ì¤‘"):
        try:
            with open(path, "rb") as f:
                file_bytes = io.BytesIO(f.read())
            # loader = CustomDocumentLoader(file_path=path,file=file_bytes, file_name=path)
            loader = HybridLLMPdfLoader(file_path=path,file=file_bytes, file_name=path)
            docs = loader.load()
            if docs:  # ë¬¸ì„œê°€ ì‹¤ì œë¡œ ìƒì„±ëœ ê²½ìš°ë§Œ ì„±ê³µìœ¼ë¡œ ì¹´ìš´íŠ¸
                all_documents.extend(docs)
                success_count += 1
            else:
                # ë¹ˆ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ëŠ” ì†ìƒëœ PDFë¡œ ê°„ì£¼
                error_count += 1
                error_files.append(path)
                logging.warning(f"âš ï¸ íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì†ìƒëœ PDF ê°€ëŠ¥): {os.path.basename(path)}")
        except Exception as e:
            error_count += 1
            error_files.append(path)
            logging.error(f"âŒ {os.path.basename(path)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}")

    # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë° ì˜¤ë¥˜ íŒŒì¼ ì €ì¥
    logging.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ - ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨/ê±´ë„ˆëœ€: {error_count}ê°œ")
    if error_files:
        logging.warning(f"âš ï¸ ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒŒì¼ {len(error_files)}ê°œ (ì†ìƒëœ PDF ë“±)")
        if len(error_files) <= 10:  # 10ê°œ ì´í•˜ë©´ ëª¨ë‘ ì¶œë ¥
            for err_file in error_files:
                logging.warning(f"   - {os.path.basename(err_file)}")
        else:  # 10ê°œ ì´ˆê³¼ë©´ ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            for err_file in error_files[:10]:
                logging.warning(f"   - {os.path.basename(err_file)}")
            logging.warning(f"   ... ì™¸ {len(error_files) - 10}ê°œ")
        
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒŒì¼ ëª©ë¡ì„ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥
        error_log_path = os.path.join(index_path, "failed_files.txt")
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with open(error_log_path, "a", encoding="utf-8") as f:
                f.write(f"\n{'='*80}\n")
                f.write(f"ì²˜ë¦¬ ì¼ì‹œ: {timestamp}\n")
                f.write(f"ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒŒì¼ ìˆ˜: {len(error_files)}ê°œ\n")
                f.write(f"{'='*80}\n")
                for err_file in error_files:
                    # ì „ì²´ ê²½ë¡œì™€ íŒŒì¼ëª… ëª¨ë‘ ì €ì¥
                    f.write(f"{err_file}\n")
            logging.info(f"ğŸ“ ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒŒì¼ ëª©ë¡ì´ '{error_log_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.error(f"ì˜¤ë¥˜ íŒŒì¼ ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")

    if not all_documents:
        logging.warning("ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì´ {len(all_documents)}ê°œì˜ ìƒˆ ë¬¸ì„œë¥¼ ì„ë² ë”© ì¤‘...")

    # ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
    if vectorstore:
        for i in tqdm(range(0, len(all_documents), batch_size), desc="ë¬¸ì„œ ì¶”ê°€ ì¤‘"):
            torch.mps.empty_cache()
            batch = all_documents[i:i + batch_size]
            vectorstore.add_documents(batch)
    else:
        first_batch = all_documents[:batch_size]
        vectorstore = FAISS.from_documents(first_batch, embeddings)
        for i in tqdm(range(batch_size, len(all_documents), batch_size), desc="ì¸ë±ìŠ¤ ìƒì„± ì¤‘"):
            torch.mps.empty_cache()
            batch = all_documents[i:i + batch_size]
            vectorstore.add_documents(batch)

    vectorstore.save_local(index_path)
    logging.info(f"ë²¡í„°ìŠ¤í† ì–´ê°€ '{index_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    input_folder = "failed_files/b"  
    append_to_vectorstore(input_folder, index_path="vectordb-failed-files-b", batch_size=16)
