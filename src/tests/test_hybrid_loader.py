"""
Hybrid LLM Document Loader í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python test_hybrid_loader.py <PDF_íŒŒì¼_ê²½ë¡œ>

ì˜ˆì‹œ:
    python test_hybrid_loader.py /Volumes/SSD_black/í•œì „/ì›ë³¸ë¬¸ì„œ/sample.pdf
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.handler.hybrid_llm_document_loader import HybridLLMPdfLoader
import json


def test_pdf_parsing(pdf_path: str, max_preview_length: int = 500):
    """
    PDF íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        max_preview_length: ê° ì²­í¬ì˜ ë¯¸ë¦¬ë³´ê¸° ìµœëŒ€ ê¸¸ì´
    """
    if not os.path.exists(pdf_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return
    
    file_name = os.path.basename(pdf_path)
    print(f"\n{'='*80}")
    print(f"ğŸ“„ PDF íŒŒì¼: {file_name}")
    print(f"ğŸ“‚ ê²½ë¡œ: {pdf_path}")
    print(f"{'='*80}\n")
    
    # ë¡œë” ìƒì„±
    print("ğŸ”§ HybridLLMPdfLoader ì´ˆê¸°í™” ì¤‘...")
    loader = HybridLLMPdfLoader(
        file_path=pdf_path,
        file=pdf_path,
        file_name=file_name,
        # llm_model="us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        llm_model="global.anthropic.claude-sonnet-4-5-20250929-v1:0",
        aws_region="us-east-1"
    )
    
    # ë¬¸ì„œ ë¡œë“œ
    print("ğŸ“– PDF íŒŒì‹± ì‹œì‘...\n")
    documents = loader.load()
    
    print(f"\n{'='*80}")
    print(f"âœ… íŒŒì‹± ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(documents)}ê°œì˜ Document ìƒì„±ë¨")
    print(f"{'='*80}\n")
    
    # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”
    pages_data = {}
    for doc in documents:
        page_num = doc.metadata.get("page", 0)
        if page_num not in pages_data:
            pages_data[page_num] = []
        pages_data[page_num].append(doc)
    
    # í˜ì´ì§€ë³„ í†µê³„ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š í˜ì´ì§€ë³„ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("="*80 + "\n")
    
    for page_num in sorted(pages_data.keys()):
        docs_in_page = pages_data[page_num]
        extract_mode = docs_in_page[0].metadata.get("extract_mode", "unknown")
        table_count = docs_in_page[0].metadata.get("table_count", 0)
        
        # ì²˜ë¦¬ ë°©ì‹ í‘œì‹œ
        if "llm" in extract_mode:
            method_icon = "ğŸ¤–"
            method_name = "LLM ì²˜ë¦¬"
        else:
            method_icon = "ğŸ“"
            method_name = "pdfplumber ì²˜ë¦¬"
        
        print(f"{method_icon} í˜ì´ì§€ {page_num:2d}: {method_name:20s} | "
              f"í…Œì´ë¸” {table_count}ê°œ | ì²­í¬ {len(docs_in_page):2d}ê°œ | "
              f"ëª¨ë“œ: {extract_mode}")
    
    # ìƒì„¸ ë‚´ìš© ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“ í˜ì´ì§€ë³„ ìƒì„¸ ë‚´ìš©")
    print("="*80 + "\n")
    
    for page_num in sorted(pages_data.keys()):
        docs_in_page = pages_data[page_num]
        extract_mode = docs_in_page[0].metadata.get("extract_mode", "unknown")
        table_count = docs_in_page[0].metadata.get("table_count", 0)
        
        print(f"\n{'â”€'*80}")
        print(f"ğŸ“„ í˜ì´ì§€ {page_num} (ì²­í¬ {len(docs_in_page)}ê°œ)")
        print(f"{'â”€'*80}")
        print(f"  ì²˜ë¦¬ ë°©ì‹: {extract_mode}")
        print(f"  í…Œì´ë¸” ê°œìˆ˜: {table_count}")
        print(f"  ì²­í¬ ê°œìˆ˜: {len(docs_in_page)}")
        
        for idx, doc in enumerate(docs_in_page, 1):
            content = doc.page_content
            
            # íŒŒì¼ëª… ì œê±° (íŒŒì¼ëª…\n\nì‹¤ì œë‚´ìš© í˜•ì‹)
            if "\n\n" in content:
                _, actual_content = content.split("\n\n", 1)
            else:
                actual_content = content
            
            # ë¯¸ë¦¬ë³´ê¸°
            preview = actual_content[:max_preview_length]
            if len(actual_content) > max_preview_length:
                preview += "..."
            
            print(f"\n  [{idx}ë²ˆì§¸ ì²­í¬] (ê¸¸ì´: {len(actual_content)}ì)")
            print(f"  â”Œ{'â”€'*76}â”")
            for line in preview.split("\n"):
                print(f"  â”‚ {line[:74]:<74s} â”‚")
            print(f"  â””{'â”€'*76}â”˜")
    
    # í†µê³„ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í†µê³„")
    print("="*80 + "\n")
    
    llm_pages = sum(1 for p in pages_data.values() if "llm" in p[0].metadata.get("extract_mode", ""))
    plumber_pages = len(pages_data) - llm_pages
    total_chunks = len(documents)
    
    print(f"  ì´ í˜ì´ì§€ ìˆ˜: {len(pages_data)}")
    print(f"  ğŸ¤– LLM ì²˜ë¦¬ í˜ì´ì§€: {llm_pages}")
    print(f"  ğŸ“ pdfplumber ì²˜ë¦¬ í˜ì´ì§€: {plumber_pages}")
    print(f"  ğŸ“¦ ì´ ì²­í¬ ìˆ˜: {total_chunks}")
    
    # í…Œì´ë¸” ê°œìˆ˜ë³„ í†µê³„
    table_stats = {}
    for docs_in_page in pages_data.values():
        count = docs_in_page[0].metadata.get("table_count", 0)
        table_stats[count] = table_stats.get(count, 0) + 1
    
    print(f"\n  í…Œì´ë¸” ê°œìˆ˜ë³„ í˜ì´ì§€ ë¶„í¬:")
    for count in sorted(table_stats.keys()):
        print(f"    í…Œì´ë¸” {count}ê°œ: {table_stats[count]}í˜ì´ì§€")
    
    print("\n" + "="*80 + "\n")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
    save_json = input("ğŸ“ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if save_json == 'y':
        # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = "/Volumes/SSD_black/í•œì „/parsing_result"
        os.makedirs(output_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (PDF íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° í›„ _parsed_result.json ì¶”ê°€)
        base_name = os.path.splitext(file_name)[0]
        output_filename = f"{base_name}_parsed_result.json"
        output_path = os.path.join(output_dir, output_filename)
        
        result = {
            "file_name": file_name,
            "total_pages": len(pages_data),
            "total_chunks": total_chunks,
            "llm_pages": llm_pages,
            "plumber_pages": plumber_pages,
            "pages": []
        }
        
        for page_num in sorted(pages_data.keys()):
            docs_in_page = pages_data[page_num]
            page_info = {
                "page_num": page_num,
                "extract_mode": docs_in_page[0].metadata.get("extract_mode"),
                "table_count": docs_in_page[0].metadata.get("table_count"),
                "chunks": []
            }
            
            for doc in docs_in_page:
                content = doc.page_content
                if "\n\n" in content:
                    _, actual_content = content.split("\n\n", 1)
                else:
                    actual_content = content
                
                page_info["chunks"].append({
                    "content": actual_content,
                    "length": len(actual_content),
                    "metadata": doc.metadata
                })
            
            result["pages"].append(page_info)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}\n")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python test_hybrid_loader.py <PDF_íŒŒì¼_ê²½ë¡œ>")
        print("ì˜ˆì‹œ: python test_hybrid_loader.py /Volumes/SSD_black/í•œì „/ì›ë³¸ë¬¸ì„œ/sample.pdf")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    test_pdf_parsing(pdf_path)

